# =================================================================
# EPay Common Chart - prod-dc Environment Configuration
# =================================================================
# Global values for all charts###
global:
  environment: prod-dc
  clusterDomain: prod.epay.sbi
  imageRegistry: proddc-registry-route-quay-enterprise.apps.dc.prod.epay.sbi:8443
  imagePullSecrets: microservices-prod-pull-secret
  storageClass: thin-csi-retain
# ================================================================
# Service Mesh 3.0 Configuration (prod-dc Instance)
# ================================================================
serviceMesh:
  enabled: true
  # Core Istio Control Plane
  controlPlane:
    name: "prod-dc-istio"
    namespace: "prod-dc-istio-system"
    version: "v1.24-latest"
    updateStrategy: "InPlace"
    meshId: "prod-dc-mesh"
    network: "prod-dc-network"
    # Discovery selector for namespace scoping
    discoverySelector:
      key: "istio-discovery"
      value: "prod-dc-istio"
  # CNI Configuration
  cni:
    enabled: true
    createResource: true
    name: "default"
    namespace: "istio-cni"
    excludeNamespaces:
      - prod-dc-istio-system
      - prod-dc-istio-cni
      - kube-system
      - openshift-system
      - openshift-operators
      - openshift-monitoring
      - openshift-user-workload-monitoring
  # Gateway Configuration (One-time mesh-wide)
  gateway:
    # Istio Gateway resource
    istioGateway:
      name: "prod-dc-gateway"
      namespace: "prod-dc-istio-system"
      hosts:
        - "sbiepay.sbi.bank.in"
      port: 8080
    # Gateway Deployment and Service
    deployment:
      name: "prod-dc-istio-gateway"
      namespace: "prod-dc-istio-system"
      replicas: 1
      selector: "prod-dc-istio-gateway"
      revision: "prod-dc-istio"
      resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
        limits:
          cpu: "2000m"
          memory: "1024Mi"
    # Gateway Service
    service:
      name: "prod-dc-istio-gateway-service"
      type: ClusterIP
      ports:
        status: 15021
        http: 8080
        https: 8443
    # OpenShift Route
    route:
      name: "prod-dc-istio-gateway-route"
      enabled: true
      host: "sbiepay.sbi.bank.in"
      tls:
        termination: "edge"
        insecureEdgeTerminationPolicy: "None"
  # Service Mesh HPA Configuration (Environment-level)
  hpa:
    enabled: true
    # Istio Control Plane HPA
    istiod:
      enabled: true
      minReplicas: 2
      maxReplicas: 5
      targetCPUUtilizationPercentage: 70
      targetMemoryUtilizationPercentage: 80
      behavior:
        scaleUp:
          stabilizationWindowSeconds: 60
          policies:
            - type: "Percent"
              value: 50
              periodSeconds: 30
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: "Percent"
              value: 10
              periodSeconds: 60
    # Istio Gateway HPA
    gateway:
      enabled: true
      minReplicas: 2
      maxReplicas: 6
      targetCPUUtilizationPercentage: 70
      targetMemoryUtilizationPercentage: 80
      behavior:
        scaleUp:
          stabilizationWindowSeconds: 60
          policies:
            - type: "Percent"
              value: 100
              periodSeconds: 15
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: "Percent"
              value: 10
              periodSeconds: 60

# ================================================================
# Infrastructure Services (One-time mesh-wide)
# ================================================================


  # Traffic Management Configuration (Infrastructure level only)
  trafficManagement:
    virtualServices:
      enabled: true
    destinationRules:
      enabled: true
  # Security Configuration (Infrastructure level only)
  security:
    authorizationPolicies:
      enabled: true
infrastructure:
  # Infrastructure peer authentication (mesh-wide)
  peerAuthentication:
    # Disable mTLS for infrastructure components in prod-dc
    kafka:
      enabled: true
      mtlsMode: "DISABLE"
    gemfire:
      enabled: true
      mtlsMode: "DISABLE"
    managementConsoles:
      enabled: true
      mtlsMode: "DISABLE"

# ================================================================
# Security Configuration
# ================================================================
security:
  enabled: true
  scc:
    enabled: true
    name: "epay-restricted-scc"
    runAsUser:
      uidRangeMin: 1000
      uidRangeMax: 2000
    seLinux:
      level: "s0"
    fsGroup:
      min: 1000
      max: 2000
    supplementalGroups:
      min: 1000
      max: 2000
    users: []
    groups: []
  # Security contexts for containers
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1001
    runAsGroup: 1001
    fsGroup: 1001
    seccompProfile:
      type: RuntimeDefault
  containerSecurityContext:
    allowPrivilegeEscalation: false
    runAsNonRoot: true
    runAsUser: 1001
    runAsGroup: 1001
    readOnlyRootFilesystem: false
    capabilities:
      drop:
        - ALL
  # Network policies for infrastructure
  networkPolicies:
    enabled: false
  # Service accounts for infrastructure components
  serviceAccounts:
    enabled: false
#
# ================================================================#
# GemFire Configuration (Simplified to match reference)
# ================================================================
gemfire:
  enabled: false
  namespace: "prod-dc-gemfire"

  cluster:
    name: "gemfire-preprod-system"
    image: "registry.preprod.epay.sbi:8443/ubi9/pivotal-gemfire/vmware-gemfire:10.1.2"

    locators:
      terminationGracePeriodSeconds: 120
      replicas: 3
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/worker
                    operator: Exists
                  - key: workload-type
                    operator: In
                    values:
                      - gemfire
      tolerations:
        - key: "workload-type"
          operator: "Equal"
          value: "gemfire"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/disk-pressure"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300  

    servers:
      terminationGracePeriodSeconds: 120
      replicas: 3
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/worker
                    operator: Exists
                  - key: workload-type
                    operator: In
                    values:
                      - gemfire
      tolerations:
        - key: "workload-type"
          operator: "Equal"
          value: "gemfire"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/disk-pressure"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300

  managementConsole:
    route:
      enabled: true
      host: "gemfire-management-console.apps.preprod.epay.sbi"
      tls:
        termination: "edge"
        insecureEdgeTerminationPolicy: "Redirect"

    enabled: true
    name: prod-dc-gemfire-management-console
    image: registry.preprod.epay.sbi:8443/ubi9/gemfire-management-console/gemfire-management-console:latest
    replicas: 1
    port: 8080
    pvcName: gemfire-data-prod-dc-gmc-0
  env:
    - name: server.port
      value: "8080"
    - name: GMC_WEBSOCKET_SENDBUFFERSIZELIMIT
      value: "5120000"
    - name: PROMETHEUS_SCRAPE_INTERVAL
      value: "2s"
  resources:
    requests:
      storage: 4Gi
  volume:
    name: gemfire-data
    mountPath: /opt/gemfire/VMware_GemFire_Management_Console
    storageClassName: thin-csi-retain
    terminationGracePeriodSeconds: 120

    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: Exists
                - key: workload-type
                  operator: In
                  values:
                    - gemfire

    tolerations:
      - key: "workload-type"
        operator: "Equal"
        value: "gemfire"
        effect: "NoSchedule"
      - key: "node.kubernetes.io/disk-pressure"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 300
    # Route configuration
    #route:
    #  enabled: true
    #  host: "gemfire-management-console.apps.preprod.epay.sbi"
    #  tls:
    #    termination: "edge"
    #    insecureEdgeTerminationPolicy: "Redirect"

# ================================================================#
# Kafka Configuration (AMQ Streams - KRaft JBOD)
# ================================================================
kafka:
  enabled: true
  namespace: "prod-dc-kafka"
  # Kafka Cluster Configuration (KRaft mode)
  cluster:
    name: "prod-dc-kafka-cluster"
    version: "3.9.0"
    metadataVersion: "3.9-IV0"
    # Annotations
    annotations:
      strimzi.io/node-pools: "enabled"
      strimzi.io/kraft: "enabled"
    # Kafka configuration
    kafka:
      listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
      - name: external
        port: 9094
        type: route
        tls: true  
      config:
        offsets.topic.replication.factor: 3
        transaction.state.log.replication.factor: 3
        transaction.state.log.min.isr: 2
        default.replication.factor: 3
        min.insync.replicas: 2
        # PDF Recommendations - Additional config
        acks: "all"
        # Performance tuning
        num.network.threads: 8
        num.io.threads: 8
        socket.send.buffer.bytes: 102400
        socket.receive.buffer.bytes: 102400
        socket.request.max.bytes: 104857600
        log.retention.hours: 168
        log.segment.bytes: 1073741824
        log.retention.check.interval.ms: 300000
        log.cleanup.policy: "delete"
        compression.type: "producer"
    # Controller node pool (PDF: At least 3, 4+ vCPU, 8GB+ RAM, 64GB+ disk)
    controller:
      name: "controller"
      replicas: 3
      roles:
        - controller
      # PDF Recommendation: Resource specifications for controllers
      resources:
        requests:
          memory: "8Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "8"
      storage:
        type: jbod
        volumes:
          - id: 0
            type: persistent-claim
            size: 100Gi  # Keep existing size to avoid warning
            kraftMetadata: shared
            deleteClaim: false
      # PDF Recommendation: terminationGracePeriodSeconds >60s
      terminationGracePeriodSeconds: 120
      # Node affinity for controllers
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/worker
                operator: Exists
              - key: workload-type
                operator: In
                values: ["kafka"]
      # Tolerations for controllers
      tolerations:
        - key: "workload-type"
          operator: "Equal"
          value: "kafka"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/disk-pressure"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300
    # Broker node pool (PDF: At least 3, 12+ vCPU, 64GB+ RAM, 1TB+ disk)
    broker:
      name: "broker"
      replicas: 3
      roles:
        - broker
      # PDF Recommendation: Resource specifications for brokers
      resources:
        requests:
          memory: "64Gi"
          cpu: "12"
        limits:
          memory: "128Gi"
          cpu: "24"
      storage:
        type: jbod
        volumes:
          - id: 0
            type: persistent-claim
            size: 100Gi  # Keep existing size to avoid warning
            kraftMetadata: shared
            deleteClaim: false
          - id: 1
            type: persistent-claim
            size: 100Gi  # Keep existing size to avoid warning
            deleteClaim: false
      # PDF Recommendation: terminationGracePeriodSeconds >60s
      terminationGracePeriodSeconds: 120
      # Node affinity for brokers
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/worker
                operator: Exists
              - key: workload-type
                operator: In
                values: ["kafka"]
      # Tolerations for brokers
      tolerations:
        - key: "workload-type"
          operator: "Equal"
          value: "kafka"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/disk-pressure"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300
        - key: "node.kubernetes.io/memory-pressure"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300

  # PDF Recommendation: Cruise Control for cluster rebalancing
  cruiseControl:
    enabled: true
    name: "prod-dc-kafka-cruise-control"
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "4Gi"
        cpu: "2"
    config:
      # Cruise Control configuration
      default.goals: >
        com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuCapacityGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.PotentialNwOutGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuUsageDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.TopicReplicaDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderReplicaDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderBytesInDistributionGoal
      cpu.balance.threshold: "1.1"
      disk.balance.threshold: "1.1"
      network.inbound.balance.threshold: "1.1"
      network.outbound.balance.threshold: "1.1"

  # PDF Recommendation: Kafka Connect for database integration
  kafkaConnect:
    enabled: false
    name: "prod-dc-kafka-connect"
    replicas: 3
    bootstrapServers: "prod-dc-kafka-cluster-kafka-bootstrap.prod-dc-kafka.svc.cluster.local:9093"
    image: "epaynonprod-registry-quay-quay-enterprise.apps.preprod.epay.sbi/redhatdefultimage/debeziumconnect:01"
    imagePullSecrets: 
       - microservices-preprod-pull-secret
    resources:
      requests:
        memory: "4Gi"
        cpu: "2"
      limits:
        memory: "8Gi"
        cpu: "4"
    config:
      group:
        id: "prod-dc-connect-cluster"
      offset:
        storage:
          topic: "prod-dc-connect-cluster-offsets"
        flush:
          interval:
            ms: 10000
          timeout:
            ms: 5000
      config:
        storage:
          topic: "prod-dc-connect-cluster-configs"
      status:
        storage:
          topic: "prod-dc-connect-cluster-status"
    # Connectors configuration

    connectors:
      - name: "epay-oracle-source"
        config:
          connector:
            class: "io.debezium.connector.oracle.OracleConnector"
          table:
            include:
              list: "PAYAGGADMIN.BANKBRANCHES,PAYAGGADMIN.AGGMERCHGTWFEESTRUCTURE,PAYAGGADMIN.SERVICETAXSTRUCTURE,PAYAGGADMIN.AGGMERCHANTSERVICETAXMAPPING,PAYAGGADMIN.AGGMERCHANTHYBRIDFEESTRUCTURE"
          database:
            hostname: "EPAYDNSPREPRODNEW.sbiepay.sbi"
            port: "1524"
            user: "PAYAGGADMIN"
            password: "April_2025"
            dbname: "epaydbpp"
            server:
              name: "epay-preprod-db"
          decimal.handling.mode: string
          schema.history.internal.skip.unparseable.ddl: "false"
          schema.history.internal.kafka.topic: schema-changes.inventory
          topic:
             prefix: cdc-oracle-gemfire     
          datetime:
            handling.mode: string      
          schema:
            include:
              list: PAYAGGADMIN
            history:
              internal:
                kafka:
                  bootstrap:
                    servers: "prod-dc-kafka-cluster-kafka-bootstrap.prod-dc-kafka.svc.cluster.local:9093"  
                topic: "epay-db-history"
          include:
            schema:
              changes: true

  # PDF Recommendation: MirrorMaker 2 for disaster recovery
 # PDF Recommendation: MirrorMaker 2 for disaster recovery
  mirrorMaker2:
    enabled: true
    name: "prod-dc-kafka-mirror-maker"
    # Active/Passive setup for DR#
    clusters:
      - alias: "source"
        bootstrapServers: "prod-dc-kafka-cluster-kafka-bootstrap.prod-dc-kafka.svc.cluster.local:9092"
      - alias: "target"
        bootstrapServers: "prod-dr-kafka-cluster-kafka-bootstrap-prod-dr-kafka.apps.dr.prod.epay.sbi:443"
        #bootstrapServers: "prod-dr-kafka-cluster-kafka-bootstrap.prod-dr-kafka.svc.cluster.local:9092"

    mirrors:
      - sourceCluster: "source"
        targetCluster: "target"
        sourceConnector:
          #tasksMax: 1 #(to increase or Maximum partition count)
          config:
            replication.factor: 3
            offset-syncs.topic.replication.factor: 3
            sync.topic.acls.enabled: true
            replication.policy.class: "org.apache.kafka.connect.mirror.IdentityReplicationPolicy"
        heartbeatConnector:
           config: {}
        #     heartbeats.topic.replication.factor: 3
        #     replication.policy.class: org.apache.kafka.connect.mirror.IdentityReplicationPolicy
        checkpointConnector:
          #tasksMax: 1 #(to increase or Maximum partition count)
          config:
            checkpoints.topic.replication.factor: 3
            refresh.groups.interval.seconds: 30
            offset.sync.interval.ms: 3000
            #sync.group.offsets.enabled: true
            checkpoints.enabled: true              # ✓ Enables checkpoint connector
            emit.checkpoints.enabled: true         # ✓ Allows sync emission
            emit.checkpoints.interval.seconds: 10  # Interval to sync offsets
            sync.group.offsets.enabled: true       # ✓ Sync group offsets
            sync.group.offsets.interval.ms: 5000   # Interval for group offset syncing
            offset.lag.max: 0
            replication.policy.class: org.apache.kafka.connect.mirror.IdentityReplicationPolicy

        topicsPattern: "merchant_.*,transaction_.*,ops_.smm_.*"
        groupsPattern: "operation-consumers,transaction-consumers,merchant-consumers,smm-test-group"
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "4Gi"
        cpu: "2"
  # Kafka Users for ePay application    
  kafkaUser:
    enabled: true
    name: epay-user
    namespace: prod-dc-kafka
    cluster: prod-dc-kafka-cluster
    authentication:
      type: scram-sha-512  # Kafka Users for ePay application  


# Kafka Topics for ePay application
topics:
  - name: "prod-dc-connect-cluster-offsets"
    partitions: 25
    replicas: 3
  - name: "prod-dc-connect-cluster-configs"
    partitions: 1
    replicas: 3
  - name: "prod-dc-connect-cluster-status"
    partitions: 5
    replicas: 3
  - name: "epay-db-history"
    partitions: 1
    replicas: 3

# PDF Recommendation: Distributed tracing for environment
tracing:
  enabled: true
  type: "tempo"
  # Create our own Tempo instance for prod-dc environment
  tempo:
    enabled: false
    name: "prod-dc-tempo"
    namespace: "prod-dc-tempo"
    # Tempo instance configuration for latest operator
    spec:
      # Storage configuration
      storage:
        secret:
          name: "tempo-s3-storage-secret"
          type: "s3"
      # Retention policy
      retention:
        global:
          traces: "168h"  # 7 days
      # Resource configuration
      resources:
        total:
          limits:
            memory: "2Gi"
            cpu: "1000m"
          requests:
            memory: "1Gi" 
            cpu: "500m"
      # Service configuration for prod-dc
      services:
        distributor:
          replicas: 1
        ingester:
          replicas: 1
        querier:
          replicas: 1
        query_frontend:
          replicas: 1
        compactor:
          replicas: 1
    # Persistence for our Tempo instance
    persistence:
      enabled: true
      size: "10Gi"
      storageClass: "thin-csi-retain"
# Environment-level monitoring infrastructure
monitoring:
  enabled: false
  
  # Use OpenShift User Workload Monitoring (built-in Prometheus)
  userWorkloadMonitoring:
    enabled: true
    namespace: "openshift-user-workload-monitoring"
    
  # Disable separate Prometheus deployment - use OpenShift built-in
  prometheus:
    enabled: true
    retention: "30d"
    
  # Grafana for enhanced dashboards (optional - OpenShift Console has basic monitoring)
  grafana:
    enabled: true
    persistence:
      enabled: true
      size: "20Gi"
      storageClass: "thi-csi"
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1"
    
  # ServiceMonitor for infrastructure components (required for OpenShift monitoring)
  serviceMonitor:
    enabled: true
    labels:
      app: "epay-infrastructure-monitoring"
    endpoints:
      - port: "metrics"
        interval: "30s"
        path: "/metrics"
    
  # PrometheusRule for infrastructure alerting (works with OpenShift monitoring)
  prometheusRule:
    enabled: true
    rules:
      - alert: "KafkaClusterDown"
        expr: "kafka_server_replicamanager_leadercount == 0"
        for: "5m"
        labels:
          severity: "critical"
        annotations:
          summary: "Kafka cluster is down"
          description: "Kafka cluster has no active leaders"
      
      - alert: "TempoDown"
        expr: "up{job='tempo'} == 0"
        for: "5m"
        labels:
          severity: "critical"
        annotations:
          summary: "Tempo tracing is down"
          description: "Tempo distributed tracing service is unavailable"

# Environment-level persistent storage
persistentStorage:
  enabled: true
  
  # Storage classes configuration
  storageClasses:
    default: "thin-csi-retain"
  
  # Persistent volumes for infrastructure components
  volumes:
    # Tempo storage
    tempo:
      enabled: true
      size: "100Gi"
      storageClass: "thin-csi-retain"
      accessMode: "ReadWriteOnce"
      
    # Prometheus storage (disabled - using OpenShift User Workload Monitoring)
    prometheus:
      enabled: true
      size: "200Gi"
      storageClass: "thin-csi-retain"
      accessMode: "ReadWriteOnce"      
    # Grafana storage
    grafana:
      enabled: true
      size: "100Gi"
      storageClass: "thin-csi-retain"
      accessMode: "ReadWriteOnce"
      
    # Kafka storage
    kafka:
      enabled: true
      size: "100Gi"
      storageClass: "thin-csi-retain"
      accessMode: "ReadWriteOnce"
      
    # Environment logs storage
    environment-logs:
      enabled: true
      size: "100Gi"
      storageClass: "thin-csi-retain"
      accessMode: "ReadWriteMany"
      
    # Backup storage
    backup:
      enabled: true
      size: "500Gi"
      storageClass: "thin-csi-retain"
      accessMode: "ReadWriteOnce"

# OpenTelemetry configuration for environment
opentelemetry:
  enabled: false
  
  # OpenTelemetry Collector
  collector:
    enabled: true
    replicas: 2
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1"
    
    # Collector configuration
    config:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: "0.0.0.0:4317"
            http:
              endpoint: "0.0.0.0:4318"
        jaeger:
          protocols:
            grpc:
              endpoint: "0.0.0.0:14250"
            thrift_http:
              endpoint: "0.0.0.0:14268"
      
      processors:
        batch:
          timeout: "1s"
          send_batch_size: 1024
        memory_limiter:
          limit_mib: 1024
      
      exporters:
        tempo:
          endpoint: "tempo-distributor.prod-dc-tempo.svc.cluster.local:4317"
          tls:
            insecure: true
      
      service:
        pipelines:
          traces:
            receivers: ["otlp", "jaeger"]
            processors: ["memory_limiter", "batch"]
            exporters: ["tempo"]
  
###
