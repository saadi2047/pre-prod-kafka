# ================================================================
# EPay Common Chart - Pre-Prod Environment Configuration
# ================================================================
# Global values for all charts#
global:
  environment: preproddc
  clusterDomain: preprod.epay.sbi
  imageRegistry: registry.preprod.epay.sbi:8443
  imagePullSecrets: []
  storageClass: trident-csi-retain
# ================================================================
# Service Mesh 3.0 Configuration (Pre-Prod-dc Instance) 
# ================================================================#
serviceMesh:
  enabled: true
  # Core Istio Control Plane
  controlPlane:
    name: "pre-prod-dc-istio"
    namespace: "pre-prod-dc-istio-system"
    version: "v1.24-latest"
    updateStrategy: "InPlace"
    meshId: "pre-prod-dc-mesh"
    network: "pre-prod-dc-network"
    # Discovery selector for namespace scoping
    discoverySelector:
      key: "istio-discovery"
      value: "pre-prod-dc-istio"
  # CNI Configuration
  cni:
    enabled: true
    createResource: true
    name: "default"
    namespace: "istio-cni"
    excludeNamespaces:
      - pre-prod-dc-istio-system
      - pre-prod-dc-istio-cni
      - kube-system
      - openshift-system
      - openshift-operators
      - openshift-monitoring
      - openshift-user-workload-monitoring
  # Gateway Configuration (One-time mesh-wide)
  gateway:
    # Istio Gateway resource
    istioGateway:
      name: "pre-prod-dc-gateway"
      namespace: "pre-prod-dc-istio-system"
      hosts:
        - "preprod.epay.sbi"
      port: 8080
    # Gateway Deployment and Service
    deployment:
      name: "pre-prod-dc-istio-gateway"
      namespace: "pre-prod-dc-istio-system"
      replicas: 1
      selector: "pre-prod-dc-istio-gateway"
      revision: "pre-prod-dc-istio"
      resources:
        requests:
          cpu: "1000m"
          memory: "512Mi"
        limits:
          cpu: "2000m"
          memory: "1024Mi"
    # Gateway Service
    service:
      name: "pre-prod-dc-istio-gateway-service"
      type: ClusterIP
      ports:
        status: 15021
        http: 8080
        https: 8443
    # OpenShift Route
    route:
      name: "pre-prod-dc-istio-gateway-route"
      enabled: true
      host: "preprod.epay.sbi"
      tls:
        termination: "edge"
        insecureEdgeTerminationPolicy: "None"
  # Service Mesh HPA Configuration (Environment-level)
  hpa:
    enabled: true
    # Istio Control Plane HPA
    istiod:
      enabled: true
      minReplicas: 2
      maxReplicas: 6
      targetCPUUtilizationPercentage: 70
      targetMemoryUtilizationPercentage: 80
      behavior:
        scaleUp:
          stabilizationWindowSeconds: 60
          policies:
            - type: "Percent"
              value: 50
              periodSeconds: 30
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: "Percent"
              value: 10
              periodSeconds: 60
    # Istio Gateway HPA
    gateway:
      enabled: true
      minReplicas: 2
      maxReplicas: 6
      targetCPUUtilizationPercentage: 70
      targetMemoryUtilizationPercentage: 80
      behavior:
        scaleUp:
          stabilizationWindowSeconds: 60
          policies:
            - type: "Percent"
              value: 100
              periodSeconds: 15
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: "Percent"
              value: 10
              periodSeconds: 60

# ================================================================
# Infrastructure Services (One-time mesh-wide)
# ================================================================


  # Traffic Management Configuration (Infrastructure level only)#
  trafficManagement:
    virtualServices:
      enabled: true
    destinationRules:
      enabled: true
  # Security Configuration (Infrastructure level only)
  security:
    authorizationPolicies:
      enabled: true
infrastructure:
  # Infrastructure peer authentication (mesh-wide)
  peerAuthentication:
    # Disable mTLS for infrastructure components in pre-prod
    kafka:
      enabled: true
      mtlsMode: "DISABLE"
    gemfire:
      enabled: false
      mtlsMode: "DISABLE"
    managementConsoles:
      enabled: true
      mtlsMode: "DISABLE"

# ================================================================
# Security Configuration
# ================================================================
security:
  enabled: false
  scc:
    enabled: false
    name: "epay-restricted-scc"
    runAsUser:
      uidRangeMin: 1000
      uidRangeMax: 2000
    seLinux:
      level: "s0"
    fsGroup:
      min: 1000
      max: 2000
    supplementalGroups:
      min: 1000
      max: 2000
    users: []
    groups: []
  # Security contexts for containers
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1001
    runAsGroup: 1001
    fsGroup: 1001
    seccompProfile:
      type: RuntimeDefault
  containerSecurityContext:
    allowPrivilegeEscalation: false
    runAsNonRoot: true
    runAsUser: 1001
    runAsGroup: 1001
    readOnlyRootFilesystem: false
    capabilities:
      drop:
        - ALL
  # Network policies for infrastructure
  networkPolicies:
    enabled: false
  # Service accounts for infrastructure components
  serviceAccounts:
    enabled: false

#================================================================#
#GemFire Configuration (Simplified to match reference)
#================================================================
gemfire:
  enabled: false
  namespace: "pre-prod-dc-gemfire"

  cluster:
    name: "gemfire-preproddc-system"
    image: "registry.preprod.epay.sbi:8443/ubi9/pivotal-gemfire/vmware-gemfire:10.1.2"

    locators:
      terminationGracePeriodSeconds: 120
      replicas: 3
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/worker
                    operator: Exists
                  - key: workload-type
                    operator: In
                    values:
                      - gemfire
      tolerations:
        - key: "workload-type"
          operator: "Equal"
          value: "gemfire"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/disk-pressure"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300  

    servers:
      terminationGracePeriodSeconds: 120
      replicas: 3
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/worker
                    operator: Exists
                  - key: workload-type
                    operator: In
                    values:
                      - gemfire
      tolerations:
        - key: "workload-type"
          operator: "Equal"
          value: "gemfire"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/disk-pressure"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300

  managementConsole:
    route:
      enabled: false
      host: "gemfire-management-console.apps.preprod.epay.sbi"
      tls:
        termination: "edge"
        insecureEdgeTerminationPolicy: "Redirect"

    enabled: false
    name: pre-prod-gmc
    image: registry.preprod.epay.sbi:8443/ubi9/gemfire-management-console/gemfire-management-console:latest
    replicas: 1
    port: 8080
    pvcName: gemfire-data-pre-prod-gmc-0
  env:
    - name: server.port
      value: "8080"
    - name: GMC_WEBSOCKET_SENDBUFFERSIZELIMIT
      value: "5120000"
    - name: PROMETHEUS_SCRAPE_INTERVAL
      value: "2s"
  resources:
    requests:
      storage: 4Gi
  volume:
    name: gemfire-data
    mountPath: /opt/gemfire/VMware_GemFire_Management_Console
    storageClassName: trident-csi-retain
    terminationGracePeriodSeconds: 120

    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: Exists
                - key: workload-type
                  operator: In
                  values:
                    - gemfire

    tolerations:
      - key: "workload-type"
        operator: "Equal"
        value: "gemfire"
        effect: "NoSchedule"
      - key: "node.kubernetes.io/disk-pressure"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 300
    #Route configuration
    route:
     enabled: true
     host: "gemfire-management-console.apps.preprod.epay.sbi"
     tls:
       termination: "edge"
       insecureEdgeTerminationPolicy: "Redirect"

# ================================================================#
# Kafka Configuration (AMQ Streams - KRaft JBOD)
# ================================================================#
kafka:
  enabled: true
  namespace: "pre-prod-dc-kafka"
  # Kafka Cluster Configuration (KRaft mode)
  cluster:
    name: "pre-prod-dc-kafka-cluster"
    version: "3.9.0"
    metadataVersion: "3.9-IV0"
    # Annotations
    annotations:
      strimzi.io/node-pools: "enabled"
      strimzi.io/kraft: "enabled"
    # Kafka configuration
    kafka:
      listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
      - name: external
        port: 9094
        type: route
        tls: true 
      config:
        offsets.topic.replication.factor: 3
        transaction.state.log.replication.factor: 3
        transaction.state.log.min.isr: 2
        default.replication.factor: 3
        min.insync.replicas: 2
        # PDF Recommendations - Additional config
        acks: "all"
        # Performance tuning
        num.network.threads: 8
        num.io.threads: 8
        socket.send.buffer.bytes: 102400
        socket.receive.buffer.bytes: 102400
        socket.request.max.bytes: 104857600
        log.retention.hours: 168
        log.segment.bytes: 1073741824
        log.retention.check.interval.ms: 300000
        log.cleanup.policy: "delete"
        compression.type: "producer"
    # Controller node pool (PDF: At least 3, 4+ vCPU, 8GB+ RAM, 64GB+ disk)
    controller:
      name: "controller"
      replicas: 3
      roles:
        - controller
      # PDF Recommendation: Resource specifications for controllers
      resources:
        requests:
          memory: "8Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "8"
      storage:
        type: jbod
        volumes:
          - id: 0
            type: persistent-claim
            size: 100Gi  # Keep existing size to avoid warning
            kraftMetadata: shared
            deleteClaim: false
      # PDF Recommendation: terminationGracePeriodSeconds >60s
      terminationGracePeriodSeconds: 120
      # Node affinity for controllers
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/worker
                operator: Exists
              - key: workload-type
                operator: In
                values: ["kafka"]
      # Tolerations for controllers
      tolerations:
        - key: "workload-type"
          operator: "Equal"
          value: "kafka"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/disk-pressure"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300
    # Broker node pool (PDF: At least 3, 12+ vCPU, 64GB+ RAM, 1TB+ disk)
    broker:
      name: "broker"
      replicas: 3
      roles:
        - broker
      # PDF Recommendation: Resource specifications for brokers
      resources:
        requests:
          memory: "64Gi"
          cpu: "12"
        limits:
          memory: "128Gi"
          cpu: "24"
      storage:
        type: jbod
        volumes:
          - id: 0
            type: persistent-claim
            size: 100Gi  # Keep existing size to avoid warning
            kraftMetadata: shared
            deleteClaim: false
          - id: 1
            type: persistent-claim
            size: 100Gi  # Keep existing size to avoid warning
            deleteClaim: false
      # PDF Recommendation: terminationGracePeriodSeconds >60s
      terminationGracePeriodSeconds: 120
      # Node affinity for brokers
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/worker
                operator: Exists
              - key: workload-type
                operator: In
                values: ["kafka"]
      # Tolerations for brokers
      tolerations:
        - key: "workload-type"
          operator: "Equal"
          value: "kafka"
          effect: "NoSchedule"
        - key: "node.kubernetes.io/disk-pressure"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300
        - key: "node.kubernetes.io/memory-pressure"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 300
  ###     
  metricsConfig:
    enabled: true 
    key: kafka-metrics-config.yml
    name: kafka-metrics 
    clusterOperator:
      path: /metrics
      port: http
    entityOperator:
      path: /metrics
      port: healthcheck
    kafkaBridge:
      path: /metrics
      port: rest-api
    kafkaResources:
      path: /metrics
      port: tcp-prometheus
      
  relabelings:
    enabled: true
    rules:
      - sourceLabels: [__meta_kubernetes_pod_label_strimzi_io_.*]
        separator: ;
        regex: (.+)
        replacement: $1
        action: labelmap
      - sourceLabels: [__meta_kubernetes_namespace]
        separator: ;
        regex: (.*)
        targetLabel: namespace
        replacement: $1
        action: replace
      - sourceLabels: [__meta_kubernetes_pod_name]
        separator: ;
        regex: (.*)
        targetLabel: kubernetes_pod_name
        replacement: $1
        action: replace
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: (.*)
        targetLabel: node_name
        replacement: $1
        action: replace
      - sourceLabels: [__meta_kubernetes_pod_host_ip]
        separator: ;
        regex: (.*)
        targetLabel: node_ip
        replacement: $1
        action: replace



  # PDF Recommendation: Cruise Control for cluster rebalancing
  cruiseControl:
    enabled: true
    name: "pre-prod-dc-kafka-cruise-control"
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "4Gi"
        cpu: "2"
    config:
      # Cruise Control configuration
      default.goals: >
        com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuCapacityGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.PotentialNwOutGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuUsageDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.TopicReplicaDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderReplicaDistributionGoal,
        com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderBytesInDistributionGoal
      cpu.balance.threshold: "1.1"
      disk.balance.threshold: "1.1"
      network.inbound.balance.threshold: "1.1"
      network.outbound.balance.threshold: "1.1"

  # PDF Recommendation: Kafka Connect for database integration
  kafkaConnect:
    enabled: false
    name: "pre-prod-dc-kafka-connect"
    replicas: 3
    bootstrapServers: "pre-prod-dc-kafka-cluster-kafka-bootstrap.pre-prod-kafka.svc.cluster.local:9093"
    image: "dc-preprod-registry-quay-quay-enterprise.apps.dcpreprod.epay.sbi/redhatdefultimage/debeziumconnect:01"
    imagePullSecrets: 
       - microservices-preprod-pull-secret
    resources:
      requests:
        memory: "4Gi"
        cpu: "2"
      limits:
        memory: "8Gi"
        cpu: "4"
    config:
      group:
        id: "pre-prod-dc-connect-cluster"
      offset:
        storage:
          topic: "pre-prod-dc-connect-cluster-offsets"
        flush:
          interval:
            ms: 10000
          timeout:
            ms: 5000
      config:
        storage:
          topic: "pre-prod-dc-connect-cluster-configs"
      status:
        storage:
          topic: "pre-prod-dc-connect-cluster-status"
    # Connectors configuration

    connectors:
      - name: "epay-oracle-source"
        config:
          connector:
            class: "io.debezium.connector.oracle.OracleConnector"
          table:
            include:
              list: "PAYAGGADMIN.BANKBRANCHES,PAYAGGADMIN.AGGMERCHGTWFEESTRUCTURE,PAYAGGADMIN.SERVICETAXSTRUCTURE,PAYAGGADMIN.AGGMERCHANTSERVICETAXMAPPING,PAYAGGADMIN.AGGMERCHANTHYBRIDFEESTRUCTURE"
          database:
            hostname: "EPAYDNSPREPRODNEW.sbiepay.sbi"
            port: "1524"
            user: "PAYAGGADMIN"
            password: "April_2025"
            dbname: "epaydbpp"
            server:
              name: "epay-preprod-dc-db"
          decimal.handling.mode: string
          schema.history.internal.skip.unparseable.ddl: "false"
          schema.history.internal.kafka.topic: schema-changes.inventory
          topic:
             prefix: cdc-oracle-gemfire     
          datetime:
            handling.mode: string      
          schema:
            include:
              list: PAYAGGADMIN
            history:
              internal:
                kafka:
                  bootstrap:
                    servers: "pre-prod-dc-kafka-cluster-kafka-bootstrap.pre-prod-dc-kafka.svc.cluster.local:9093"  
                topic: "epay-db-history"
          include:
            schema:
              changes: true

  # PDF Recommendation: MirrorMaker 2 for disaster recovery
  mirrorMaker2:
    enabled: true
    name: "pre-prod-dc-kafka-mirror-maker"
    # Active/Passive setup for DR
    clusters:
      - alias: "source"
        bootstrapServers: "pre-prod-dc-kafka-cluster-kafka-bootstrap.pre-prod-dc-kafka.svc.cluster.local:9092"
      - alias: "target"
        bootstrapServers: "pre-prod-kafka-cluster-kafka-bootstrap-pre-prod-kafka.apps.preprod.epay.sbi:443"
        #bootstrapServers: "dr-kafka-cluster-kafka-bootstrap.dr-kafka.svc.cluster.local:9092"
    mirrors:
      - sourceCluster: "source"
        targetCluster: "target"
        sourceConnector:
          config:
            replication.factor: 3
            offset-syncs.topic.replication.factor: 3
            sync.topic.acls.enabled: true
            replication.policy.class: "org.apache.kafka.connect.mirror.IdentityReplicationPolicy"
        heartbeatConnector:
          config:
            heartbeats.topic.replication.factor: 3
        checkpointConnector:
          config:
            checkpoints.topic.replication.factor: 3
        topicsPattern: ".*"
        groupsPattern: ".*"
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "4Gi"
        cpu: "2"

  # Kafka Users for ePay application    
  kafkaUser:
    enabled: true
    name: epay-user
    namespace: pre-prod-dc-kafka
    cluster: pre-prod-dc-kafka-cluster
    authentication:
      type: scram-sha-512  # Kafka Users for ePay application       
  
  console:
    enabled: true
    name: pre-prod-dc-console
    namespace: pre-prod-dc-kafka
    hostname: pre-prod-dc-kafkaconsole.apps.preprod.epay.sbi
    credentials:
      kafkaUser:
        name: epay-user
    listener: plain
    kafkaClusters:
      name: pre-prod-dc-kafka-cluster 
      namespace: pre-prod-dc-kafka


# Kafka Topics for ePay application
topics:
  - name: "pre-prod-dc-connect-cluster-offsets"
    partitions: 25
    replicas: 3
  - name: "pre-prod-dc-connect-cluster-configs"
    partitions: 1
    replicas: 3
  - name: "pre-prod-dc-connect-cluster-status"
    partitions: 5
    replicas: 3
  - name: "epay-db-history"
    partitions: 1
    replicas: 3
   

# PDF Recommendation: Distributed tracing for environment
tracing:
  enabled: false
  type: "tempo"
  # Create our own Tempo instance for pre-prod-dc environment
  tempo:
    enabled: false
    name: "pre-prod-dc-tempo"
    namespace: "pre-prod-dc-tempo"
    tenants:
       tenantName: dev
       tenantId: "1610b0c3-c509-4592-a256-a1871353dbfa"

    # Tempo instance configuration for latest operator
    spec:
      # Storage configuration
      storage:
        secret:
          name: "tempo-s3-storage-secret"
          type: "s3"
      # Retention policy
      retention:
        global:
          traces: "168h"  # 7 days
      # Resource configuration
      resources:
        total:
          limits:
            memory: "2Gi"
            cpu: "1000m"
          requests:
            memory: "1Gi" 
            cpu: "500m"
      # Service configuration for pre-prod-dc
      services:
        distributor:
          replicas: 1
        ingester:
          replicas: 1
        querier:
          replicas: 1
        query_frontend:
          replicas: 1
        compactor:
          replicas: 1
    # Persistence for our Tempo instance
    persistence:
      enabled: true
      size: "50Gi"
      storageClass: "trident-csi-retain"
# Environment-level monitoring infrastructure
monitoring:
  enabled: false
  
  # Use OpenShift User Workload Monitoring (built-in Prometheus)
  userWorkloadMonitoring:
    enabled: true
    namespace: "openshift-user-workload-monitoring"
    
  # Disable separate Prometheus deployment - use OpenShift built-in
  prometheus:
    enabled: false
    retention: "30d"
    
  # Grafana for enhanced dashboards (optional - OpenShift Console has basic monitoring)
  grafana:
    enabled: true
    persistence:
      enabled: true
      size: "20Gi"
      storageClass: "trident-csi-retain"
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1"
    
  # ServiceMonitor for infrastructure components (required for OpenShift monitoring)
  serviceMonitor:
    enabled: true
    labels:
      app: "epay-infrastructure-monitoring"
    endpoints:
      - port: "metrics"
        interval: "30s"
        path: "/metrics"
    
  # PrometheusRule for infrastructure alerting (works with OpenShift monitoring)
  prometheusRule:
    enabled: false
    rules:
      - alert: "KafkaClusterDown"
        expr: "kafka_server_replicamanager_leadercount == 0"
        for: "5m"
        labels:
          severity: "critical"
        annotations:
          summary: "Kafka cluster is down"
          description: "Kafka cluster has no active leaders"
      
      - alert: "TempoDown"
        expr: "up{job='tempo'} == 0"
        for: "5m"
        labels:
          severity: "critical"
        annotations:
          summary: "Tempo tracing is down"
          description: "Tempo distributed tracing service is unavailable"
        

  kiali:
    serviceAccount:
      name: pre-prod-dc-kiali-service-account
      namespace: pre-prod-dc-istio-system

    # Horizontal Pod Autoscaler configuration
    hpa:
      enabled: true
      minReplicas: 1
      maxReplicas: 2
      targetDeploymentName: kiali
      cpu:
        averageUtilization: 80
      memory:
        averageUtilization: 80

    deployment:
      cluster_wide_access: true
      instance_name: kiali
      logger:
        log_level: info
      view_only_mode: false

    external_services:
      tracing:
        auth:
          insecure_skip_verify: false
          type: "none"
          use_kiali_token: false
        enabled: false
        in_cluster_url: ""
        namespace_selector: false
        provider: ""
        query_timeout: 30s
        url: ""
        use_grpc: false
        
      grafana:
        enabled: false
      prometheus:
        auth:
          type: "none"
          use_kiali_token: false
        thanos_proxy:
          enabled: false
        url: ""    

    # RBAC configuration
    rbac:
      # General Kiali labels
      labels:
        app: kiali
        app.kubernetes.io/instance: pre-prod-dc-kiali
        app.kubernetes.io/name: kiali
        app.kubernetes.io/part-of: kiali

      # Annotations for the ClusterRoleBindings
      annotations:
        operator-sdk/primary-resource: pre-prod-dc-istio-system/pre-prod-kiali
        operator-sdk/primary-resource-type: Kiali.kiali.io

      clusterRoleBindings:
        # ClusterRoleBinding for 'cluster-monitoring-view'
        monitoring:
          name: pre-prod-dc-kiali-cluster-monitoring-view
          roleName: cluster-monitoring-view

        # ClusterRoleBinding for OAuth
        oauth:
          name: pre-prod-dc-kiali-pre-prod-dc-istio-system-oauth
          roleName: pre-prod-dc-kiali-pre-prod-dc-istio-system-oauth

        # ClusterRoleBinding for Kiali Viewer
        viewer:
          name: pre-prod-dc-kiali
          roleName: pre-prod-kiali-viewer


# Environment-level persistent storage
persistentStorage:
  enabled: false
  
  # Storage classes configuration
  storageClasses:
    default: "trident-csi-retain"
  
  # Persistent volumes for infrastructure components
  volumes:
    # Tempo storage
    tempo:
      enabled: false
      size: "50Gi"
      storageClass: "trident-csi-retain"
      accessMode: "ReadWriteOnce"
      
    # Prometheus storage (disabled - using OpenShift User Workload Monitoring)
    prometheus:
      enabled: true
      size: "200Gi"
      storageClass: "trident-csi-retain"
      accessMode: "ReadWriteOnce"      
    # Grafana storage
    grafana:
      enabled: true
      size: "20Gi"
      storageClass: "trident-csi-retain"
      accessMode: "ReadWriteOnce"
      
    # Kafka storage
    kafka:
      enabled: true
      size: "100Gi"
      storageClass: "trident-csi-retain"
      accessMode: "ReadWriteOnce"
      
    # Environment logs storage
    environment-logs:
      enabled: true
      size: "50Gi"
      storageClass: "trident-csi-retain"
      accessMode: "ReadWriteMany"
      
    # Backup storage
    backup:
      enabled: true
      size: "500Gi"
      storageClass: "trident-csi-retain"
      accessMode: "ReadWriteOnce"
      


# OpenTelemetry configuration for environment
opentelemetry:
  enabled: false
  
  # OpenTelemetry Collector
  collector:
    enabled: true
    replicas: 2
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1"
    
    # Collector configuration
    config:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: "0.0.0.0:4317"
            http:
              endpoint: "0.0.0.0:4318"
        jaeger:
          protocols:
            grpc:
              endpoint: "0.0.0.0:14250"
            thrift_http:
              endpoint: "0.0.0.0:14268"
      
      processors:
        batch:
          timeout: "1s"
          send_batch_size: 1024
        memory_limiter:
          limit_mib: 1024
      
      exporters:
        tempo:
          endpoint: "tempo-distributor.pre-prod-dc-tempo.svc.cluster.local:4317"
          tls:
            insecure: true
      
      service:
        pipelines:
          traces:
            receivers: ["otlp", "jaeger"]
            processors: ["memory_limiter", "batch"]
            exporters: ["tempo"]

# TempoStack Configuration
tempo:
  name: simplest
  namespace: pre-prod-dc-tempo
  storage:
    secretName: tempo-s3-storage-secret
    type: s3
  storageSize: 100Gi
  resources:
    total:
      limits:
        memory: 10Gi
        cpu: 6000m
  tenants:
    mode: openshift
    authentication:
      - tenantName: dev
        tenantId: "1610b0c3-c509-4592-a256-a1871353dbfa"
  template:
    gateway:
      enabled: true
    queryFrontend:
      jaegerQuery:
        enabled: true
#
# OpenTelemetry Collector Configuration
otelCollector:
  name: dev
  namespace: tracing-system
  image: ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:v0.92.0
  serviceAccountName: dev-collector
  tempoGateway:
    host: tempo-simplest-gateway
    grpcPort: 8090
    httpPort: 8080


################################################################
# Istio Configuration
istio:
  namespace: pre-prod-dc-istio-system
  meshConfig:
    enableTracing: true
    extensionProviders:
      - name: otel
        opentelemetry:
          port: 4317
          service: dev-collector.pre-prod-dc-istio-system.svc.cluster.local

# Kiali Configuration
kiali:
  namespace: pre-prod-dc-kiali
  deployment:
    cluster_wide_access: true
    instance_name: kiali
    logger:
      log_level: info
    view_only_mode: false
  external_services:
    tracing:
      enabled: true
      in_cluster_url: 'https://simplest-gateway.perf-tempo.svc.cluster.local:8080/api/traces/v1/dev'
      url: 'https://tempo-simplest-gateway-perf-tempo.apps.preprod.epay.sbi/api/traces/v1/dev/search'
      auth:
        type: bearer
        use_kiali_token: true
        insecure_skip_verify: true
      provider: jaeger
      query_timeout: 30
      namespace_selector: true
    grafana:
      enabled: false
    prometheus:
      thanos_proxy:
        enabled: true
      url: 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091'
      auth:
        type: bearer
        use_kiali_token: true

# Telemetry Generation Jobs
telemetrygen:
  image: ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:v0.92.0
  namespace: pre-prod-dc-istio-system
  grpc:
    enabled: true
    count: 10
  http:
    enabled: true
    count: 10

####
Security:
  serviceAccounts:
    create: true
    list:
      - name: "kiali-service-account"
        annotations:
          "my-annotation-key": "kiali monitoring"
#      - name: "my-second-serviceaccount"
#        annotations:
#          "description": "Service Account for my application's backend"
#      - name: "my-third-serviceaccount"
####
